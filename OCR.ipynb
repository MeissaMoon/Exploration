{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mathematical-generator",
   "metadata": {},
   "source": [
    "### OCR\n",
    "---\n",
    "- [What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis](https://arxiv.org/pdf/1904.01906.pdf)\n",
    "- 데이터셋\n",
    "  - [MJ Synth](https://www.robots.ox.ac.uk/~vgg/data/text/)\n",
    "  - [SynthText](https://www.robots.ox.ac.uk/~vgg/data/scenetext/)\n",
    "- pip install lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = os.path.join(os.getenv('HOME'),'aiffel/ocr')\n",
    "os.chdir(path)\n",
    "\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-panic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type=\"GPU\")\n",
    "tf.config.experimental.set_visible_devices(devices=gpus[0], device_type=\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(device=gpus[0], enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-sheriff",
   "metadata": {},
   "source": [
    "### Recognition model (1)\n",
    "- [An End-to-End Trainable Neural Network for Image-based SequenceRecognition and Its Application to Scene Text Recognition](https://arxiv.org/pdf/1507.05717.pdf)  \n",
    "![Recognition](https://aiffelstaticprd.blob.core.windows.net/media/original_images/e-23-2.crnn.png)\n",
    "![Recognition2](https://aiffelstaticprd.blob.core.windows.net/media/original_images/e-23-3.crnn_structure.png)\n",
    "- 입력이미지를 Convolution Layer를 통해 Feature를 추출하고 추출된 Feature를 얻습니다.\n",
    "- Recurrent Layer는 추출된 Feature의 전체적인 Context를 파악하고 다양한 output의 크기에 대응가능합니다.\n",
    "- Transcription layer(Fully connected layer)는 step마다 어떤 character의 확률이 높은지 예측합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영문 대문자와 숫자를 인식하기 위한 Class 개수로 36개가 필요하고, 문자가 없을 시 공백추가를 위하여 1개를 더 추가할 수 있다.\n",
    "NUMBERS = \"0123456789\"\n",
    "ENG_CHAR_UPPER = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "TARGET_CHARACTERS = ENG_CHAR_UPPER + NUMBERS\n",
    "print(f\"The total number of characters is {len(TARGET_CHARACTERS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import six\n",
    "import math\n",
    "import lmdb\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "HOME_DIR = os.getenv('HOME')+'/aiffel/ocr'\n",
    "\n",
    "# 로컬 사용자\n",
    "TRAIN_DATA_PATH = HOME_DIR+'/MJ/MJ_train'\n",
    "VALID_DATA_PATH = HOME_DIR+'/MJ/MJ_valid'\n",
    "TEST_DATA_PATH = HOME_DIR+'/MJ/MJ_test'\n",
    "\n",
    "# 클라우드 사용자는 아래 주석을 사용해 주세요.\n",
    "# TRAIN_DATA_PATH = HOME_DIR+'/data/MJ/MJ_train'\n",
    "# VALID_DATA_PATH = HOME_DIR+'/data/MJ/MJ_valid'\n",
    "# TEST_DATA_PATH = HOME_DIR+'/data/MJ/MJ_test'\n",
    "print(TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-bristol",
   "metadata": {},
   "source": [
    "### Recognition model (2) Input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "env = lmdb.open(TRAIN_DATA_PATH, max_readers=32, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "with env.begin(write=False) as txn:\n",
    "    for index in range(1, 5):\n",
    "        label_key = 'label-%09d'.encode() % index\n",
    "        label = txn.get(label_key).decode('utf-8')\n",
    "        img_key = 'image-%09d'.encode() % index\n",
    "        imgbuf = txn.get(img_key)\n",
    "        buf = six.BytesIO()\n",
    "        buf.write(imgbuf)\n",
    "        buf.seek(0)\n",
    "        try:\n",
    "            img = Image.open(buf).convert('RGB')\n",
    "\n",
    "        except IOError:\n",
    "            img = Image.new('RGB', (100, 32))\n",
    "            label = '-'\n",
    "        width, height = img.size\n",
    "        print('original image width:{}, height:{}'.format(width, height))\n",
    "        \n",
    "        target_width = min(int(width*32/height), 100)\n",
    "        target_img_size = (target_width,32 )\n",
    "        \n",
    "        print('target_img_size:{}'.format(target_img_size))\n",
    "        \n",
    "        img = np.array(img.resize(target_img_size)).transpose(1,0,2)\n",
    "       \n",
    "        print('display img shape:{}'.format(img.shape))\n",
    "        print('label:{}'.format(label))\n",
    "        display(Image.fromarray(img.transpose(1,0,2).astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MJDatasetSequence(Sequence):\n",
    "    def __init__(self, \n",
    "                      dataset_path,\n",
    "                      label_converter,\n",
    "                      batch_size=1,\n",
    "                      img_size=(100,32),\n",
    "                      max_text_len=22,\n",
    "                      is_train=False,\n",
    "                      character=''\n",
    "                ):\n",
    "        \n",
    "        self.label_converter = label_converter\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.max_text_len = max_text_len\n",
    "        self.character = character\n",
    "        self.is_train = is_train\n",
    "        self.divide_length = 100\n",
    "\n",
    "        self.env = lmdb.open(dataset_path, max_readers=32, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            num_samples = int(txn.get('num-samples'.encode()))\n",
    "            self.num_samples = int(num_samples)\n",
    "            self.index_list = [index + 1 for index in range(self.num_samples)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.is_train:\n",
    "            return math.ceil(self.num_samples/self.batch_size/self.divide_length)\n",
    "        return math.ceil(self.num_samples/self.batch_size/self.divide_length)\n",
    "    \n",
    "    # index에 해당하는 image와 label을 가져오는 메소드\n",
    "    def _get_img_label(self, index):\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            label_key = 'label-%09d'.encode() % index\n",
    "            label = txn.get(label_key).decode('utf-8')\n",
    "            img_key = 'image-%09d'.encode() % index\n",
    "            imgbuf = txn.get(img_key)\n",
    "\n",
    "            buf = six.BytesIO()\n",
    "            buf.write(imgbuf)\n",
    "            buf.seek(0)\n",
    "            try:\n",
    "                img = Image.open(buf).convert('RGB')\n",
    "\n",
    "            except IOError:\n",
    "                img = Image.new('RGB', self.img_size)\n",
    "                label = '-'\n",
    "            width, height = img.size\n",
    "            \n",
    "            target_width = min(int(width*self.img_size[1]/height), self.img_size[0])\n",
    "            target_img_size = (target_width,self.img_size[1] )\n",
    "            img = np.array(img.resize(target_img_size)).transpose(1,0,2)\n",
    "            label = label.upper()[:self.max_text_len]\n",
    "            out_of_char = f'[^{self.character}]'\n",
    "            label = re.sub(out_of_char, '', label)\n",
    "\n",
    "        return (img, label)\n",
    "    \n",
    "    # idx번째 배치를 가져오는 메소드\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indicies = self.index_list[\n",
    "            idx*self.batch_size:\n",
    "            (idx+1)*self.batch_size\n",
    "        ]\n",
    "        input_images = np.zeros([self.batch_size, *self.img_size, 3])\n",
    "        labels = np.zeros([self.batch_size, self.max_text_len], dtype='int64')\n",
    "\n",
    "        input_length = np.ones([self.batch_size], dtype='int64')*self.max_text_len\n",
    "        label_length = np.ones([self.batch_size], dtype='int64')\n",
    "\n",
    "        for i, index in enumerate(batch_indicies):\n",
    "            img, label = self._get_img_label(index)\n",
    "            encoded_label = self.label_converter.encode(label)\n",
    "            width = img.shape[0]\n",
    "            input_images[i,:width,:,:] = img\n",
    "            if len(encoded_label) > self.max_text_len:\n",
    "                continue\n",
    "            labels[i,0:len(encoded_label)] = encoded_label\n",
    "            label_length[i] = len(encoded_label)\n",
    "\n",
    "        inputs = {\n",
    "            'input_image': input_images,\n",
    "            'label': labels,\n",
    "            'input_length': input_length,\n",
    "            'label_length': label_length,\n",
    "        }\n",
    "        outputs = {'ctc': np.zeros([self.batch_size, 1])}\n",
    "        return inputs, outputs\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        self.index_list =  [index + 1 for index in range(self.num_samples)]\n",
    "        if self.is_train :\n",
    "            np.random.shuffle(self.index_list)\n",
    "            return self.index_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-background",
   "metadata": {},
   "source": [
    "### Recognition model (3) Encode\n",
    "- __init__() 에서는 입력으로 받은 text를 self.dict에 각 character들이 어떤 index에 매핑되는지 저장합니다. 이 character와 index 정보를 통해 모델이 학습할 수 있는 output이 만들어집니다. 만약 character='ABCD'라면 'A'의 label은 1, 'B'의 label은 2가 됩니다.\n",
    "- 공백(blank) 문자를 지정합니다. 여기서는 공백 문자를 뜻하기 위해 '-'를 활용하며, label은 0으로 지정합니다.\n",
    "- decode()는 각 index를 다시 character로 변환한 후 이어주어 우리가 읽을 수 있는 text로 바꾸어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelConverter(object):\n",
    "     \"\"\" Convert between text-label and text-index \"\"\"\n",
    "\n",
    "     def __init__(self, character):\n",
    "         self.character = \"-\" + character\n",
    "         self.label_map = dict()\n",
    "         for i, char in enumerate(self.character):\n",
    "             self.label_map[char] = i\n",
    "\n",
    "     def encode(self, text):\n",
    "         encoded_label = []\n",
    "         for i, char in enumerate(text):\n",
    "             if i > 0 and char == text[i - 1]:\n",
    "                 encoded_label.append(0)    # 같은 문자 사이에 공백 문자 label을 삽입\n",
    "             encoded_label.append(self.label_map[char])\n",
    "         return np.array(encoded_label)\n",
    "\n",
    "     def decode(self, encoded_label):\n",
    "         target_characters = list(self.character)\n",
    "         decoded_label = \"\"\n",
    "         for encode in encoded_label:\n",
    "             decoded_label += self.character[encode]\n",
    "         return decoded_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_converter = LabelConverter(TARGET_CHARACTERS)\n",
    "\n",
    "encdoded_text = label_converter.encode('HELLO')\n",
    "print(\"Encdoded_text: \", encdoded_text)\n",
    "decoded_text = label_converter.decode(encdoded_text)\n",
    "print(\"Decoded_text: \", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-bennett",
   "metadata": {},
   "source": [
    "동일한 영문 'L'이 연속되면 사이에 공백 문자가 포함된 것으로 확인됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-killer",
   "metadata": {},
   "source": [
    "### Recognition model (4) Build CRNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-demand",
   "metadata": {},
   "source": [
    "#### ctc_batch_cost\n",
    "- [Tensorflow Tutorial - ctc_batch_cost](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/backend/ctc_batch_cost)  \n",
    "![cost](https://aiffelstaticprd.blob.core.windows.net/media/original_images/ctc.png)  \n",
    "- CTC-Loss를 다루는 이유는 입력의 길이 T와 라벨의 길이 U의 단위가 일치하지 않을 때, 그래서 라벨은 APPLE이지만 모델이 출력한 결과는 AAAPPPPLLLLEE 처럼 나올 수 있기때문에 보정하는 로직을 추가해주었습니다.\n",
    "  \n",
    "![cost2](https://aiffelstaticprd.blob.core.windows.net/media/original_images/GC-6-P-example.png)  \n",
    "- y_true: 실제 라벨 LUBE. 텍스트 라벨 그대로가 아니라, 각 글자를 One-hot 인코딩한 형태로서, max_string_length 값은 모델에서 22로 지정할 예정\n",
    "- y_pred: 우리가 만들 RCNN 모델의 출력 결과. 그러나 길이는 4가 아니라 우리가 만들 RNN의 최종 출력 길이로서 24가 될 예정\n",
    "- input_length tensor: 모델 입력 길이 T로서, 이 경우에는 텍스트의 width인 74\n",
    "- label_length tensor: 라벨의 실제 정답 길이 U로서, 이 경우에는 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args): # CTC loss를 계산하기 위한 Lambda 함수\n",
    "    labels, y_pred, label_length, input_length = args\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_crnn_model(input_shape=(100,32,3), characters=TARGET_CHARACTERS):\n",
    "    num_chars = len(characters)+2\n",
    "    image_input = layers.Input(shape=input_shape, dtype='float32', name='input_image')\n",
    "\n",
    "    # Build CRNN model\n",
    "    conv = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(image_input)\n",
    "    conv = layers.MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "    conv = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "    conv = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.MaxPooling2D(pool_size=(1, 2))(conv)\n",
    "    conv = layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.BatchNormalization()(conv)\n",
    "    conv = layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.BatchNormalization()(conv)\n",
    "    conv = layers.MaxPooling2D(pool_size=(1, 2))(conv)     \n",
    "    feature = layers.Conv2D(512, (2, 2), activation='relu', kernel_initializer='he_normal')(conv)\n",
    "    sequnce = layers.Reshape(target_shape=(24, 512))(feature)\n",
    "    sequnce = layers.Dense(64, activation='relu')(sequnce)\n",
    "    sequnce = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(sequnce)\n",
    "    sequnce = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(sequnce)\n",
    "    y_pred = layers.Dense(num_chars, activation='softmax', name='output')(sequnce)\n",
    "\n",
    "    labels = layers.Input(shape=[22], dtype='int64', name='label')\n",
    "    input_length = layers.Input(shape=[1], dtype='int64', name='input_length')\n",
    "    label_length = layers.Input(shape=[1], dtype='int64', name='label_length')\n",
    "    loss_out = layers.Lambda(ctc_lambda_func, output_shape=(1,), name=\"ctc\")(\n",
    "        [labels, y_pred, label_length, input_length]\n",
    "    )\n",
    "    model_input = [image_input, labels, input_length, label_length]\n",
    "    model = Model(\n",
    "        inputs=model_input,\n",
    "        outputs=loss_out\n",
    "    )\n",
    "    y_func = tf.keras.backend.function(image_input, [y_pred])\n",
    "    return model, y_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-watershed",
   "metadata": {},
   "source": [
    "### Recognition model (5) Train & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MJDatasetSequence(TRAIN_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS, is_train=True)\n",
    "val_set = MJDatasetSequence(VALID_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS)\n",
    "\n",
    "checkpoint_path = HOME_DIR + '/model_checkpoint.hdf5'\n",
    "\n",
    "model, y_func = build_crnn_model()\n",
    "sgd = tf.keras.optimizers.Adadelta(lr=0.1, clipnorm=5)\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n",
    "ckp = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_loss',\n",
    "    verbose=1, save_best_only=True, save_weights_only=True\n",
    ")\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'\n",
    ")\n",
    "model.fit(train_set,\n",
    "        steps_per_epoch=len(val_set),\n",
    "        epochs=100,\n",
    "        validation_data=val_set,\n",
    "        validation_steps=len(val_set),\n",
    "        callbacks=[ckp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "test_set = MJDatasetSequence(TEST_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS)\n",
    "\n",
    "model, y_func = build_crnn_model()\n",
    "\n",
    "model.load_weights(checkpoint_path)\n",
    "input_data = model.get_layer('input_image').output\n",
    "y_pred = model.get_layer('output').output\n",
    "model_pred = Model(inputs=input_data, outputs=y_pred)\n",
    "\n",
    "def decode_predict_ctc(out, chars = TARGET_CHARACTERS, top_paths = 1):\n",
    "    results = []\n",
    "    beam_width = 5\n",
    "    if beam_width < top_paths:\n",
    "        beam_width = top_paths\n",
    "    for i in range(top_paths):\n",
    "        indexes = K.get_value(\n",
    "            K.ctc_decode(\n",
    "                out, input_length = np.ones(out.shape[0]) * out.shape[1],\n",
    "                greedy =False , beam_width = beam_width, top_paths = top_paths\n",
    "            )[0][i]\n",
    "        )[0]\n",
    "        text = \"\"\n",
    "        for index in indexes:\n",
    "            text += chars[index]\n",
    "        results.append(text)\n",
    "    return results\n",
    "\n",
    "def check_inference(model, dataset, index = 5):\n",
    "    for i in range(index):\n",
    "        inputs, outputs = dataset[i]\n",
    "        img = dataset[i][0]['input_image'][0:1,:,:,:]\n",
    "        output = model_pred.predict(img)\n",
    "        result = decode_predict_ctc(output, chars=\"-\"+TARGET_CHARACTERS)[0].replace('-','')\n",
    "        print(\"Result: \\t\", result)\n",
    "        display(Image.fromarray(img[0].transpose(1,0,2).astype(np.uint8)))\n",
    "\n",
    "check_inference(model, test_set, index=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-camel",
   "metadata": {},
   "source": [
    "### keras-ocr의 Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_ocr.detection import Detector\n",
    "SAMPLE_IMG_PATH = 'sample.jpg'\n",
    "detector = Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_draw = ImageDraw.Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text(img_path):\n",
    "    # TODO\n",
    "    # 이미지 불러오기\n",
    "    img_pil = Image.open(img_path)\n",
    "    img_pil = img_pil.resize((640, 640))\n",
    "    img_draw=ImageDraw.Draw(img_pil)\n",
    "    result_img = img_pil\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img)\n",
    "    img = tf.image.resize(img, (640, 640))\n",
    "    img = img[tf.newaxis, :, :, :]\n",
    "        # 배치 크기를 위해서 dimension을 확장해주고 kera-ocr의 입력 차원에 맞게 H,W,C로 변경합니다.\n",
    "    det_result = detector.detect(img.numpy())\n",
    "        # 배치의 첫 번째 결과만 가져옵니다.\n",
    "    ocr_result = det_result[0]\n",
    "        # 시각화를 위해서 x와 y좌표를 변경해줍니다. (앞선 h dimension으로 인해 y,x로 표기됨)\n",
    "    for idx in range(len(ocr_result)) :\n",
    "          tmp = ocr_result[idx, :, 0].copy()\n",
    "          ocr_result[idx, :, 0] = ocr_result[idx, :, 1]\n",
    "          ocr_result[idx, :, 1] = tmp\n",
    "        \n",
    "        \n",
    "    cropped_imgs = []\n",
    "    for text_result in ocr_result:\n",
    "        img_draw.polygon(text_result, outline='red')\n",
    "        x_min = text_result[:,0].min() - 5\n",
    "        x_max = text_result[:,0].max() + 5\n",
    "        y_min = text_result[:,1].min() - 5\n",
    "        y_max = text_result[:,1].max() + 5\n",
    "        word_box = [x_min, y_min, x_max, y_max]\n",
    "        cropped_imgs.append(img_pil.crop(word_box))\n",
    "\n",
    "\n",
    "    return result_img, cropped_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pil, cropped_img = detect_text(SAMPLE_IMG_PATH)\n",
    "display(img_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_img(pil_img, input_img_size=(100,32)):\n",
    "    # TODO: 잘려진 단어 이미지를 인식하는 코드를 작성하세요!\n",
    "    pil_img = pil_img.resize(input_img_size)\n",
    "    np_img = np.array(pil_img)\n",
    "    np_img = np.transpose(np_img, (1, 0, 2))\n",
    "    np_img = np_img[np.newaxis, :, :, :]\n",
    "    output = model_pred.predict(np_img)\n",
    "    result = decode_predict_ctc(output, chars=\"-\"+TARGET_CHARACTERS)[0].replace('-','')\n",
    "    print(\"Result: \\t\", result)\n",
    "    display(Image.fromarray(np.array(pil_img).astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _img in cropped_img:\n",
    "    recognize_img(_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-windows",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

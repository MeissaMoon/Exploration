{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tutorial-testimony",
   "metadata": {},
   "source": [
    "### human keypoint detection \n",
    "---\n",
    "![HP](https://aiffelstaticprd.blob.core.windows.net/media/images/03_6C5RZR1.max-800x600.png)  \n",
    "- Top-down\n",
    "  - 모든 사람의 정확한 keypoint 를 찾기 위해 object detection 을 사용\n",
    "  - crop 한 이미지 내에서 keypoint 를 찾아내는 방법으로 표현\n",
    "  - detector가 선행되어야 하고 모든 사람마다 알고리즘을 적용해야하기 때문에 사람이 많이 등장할 때는 느리다는 단점이 있다.\n",
    "- Bottom-up\n",
    "  - detector가 없고 keypoint 를 먼저 검출\n",
    "  예로 손목에 해당하는 모든 점들을 검출\n",
    "  - 한 사람에 해당하는 keypoint 를 clustering \n",
    "  - detector 가 없기 때문에 다수의 사람이 영상에 등장하더라도 속도 저하가 크지 않다. \n",
    "  - top down 방식에 비해 keypoint 검출범위가 넓어 성능이 떨어진다는 단점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-vienna",
   "metadata": {},
   "source": [
    "### Convolutional Pose Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-browse",
   "metadata": {},
   "source": [
    "CVPR 2016에서 발표된 CPM 은 completely differentiable 한 multi-stage 구조를 제안했습니다. multi stage 방법들은 DeepPose 에서부터 지속적으로 사용되어 왔었습니다.\n",
    "하지만 crop 연산 등 비연속적인 미분불가능한 stage 단위로 나눠져 있었기 때문에 학습 과정을 여러번 반복하는 비효율적인 방법을 사용해왔습니다.\n",
    "\n",
    "- [Convolutional Pose Machines](https://arxiv.org/pdf/1602.00134.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-school",
   "metadata": {},
   "source": [
    "CPM 은 end-to-end 로 학습할 수 있는 모델을 제안합니다.\n",
    "![1](https://aiffelstaticprd.blob.core.windows.net/media/images/13_wp3QD5J.max-800x600.png)   \n",
    "Stage 1 은 image feature 를 계산하는 역할을 하고 stage 2는 keypoint 를 예측하는 역할을 합니다. g1과 g2 모두 heatmap 을 출력하게 만들어서 재사용이 가능한 부분은 weight sharing 할 수 있도록 세부 모델을 설계 했습니다.\n",
    "\n",
    "![2](https://aiffelstaticprd.blob.core.windows.net/media/images/14.max-800x600.png)  \n",
    "\n",
    "Stage ≥ 2 에서 볼 수 있듯이 stage 2 이상부터는 반복적으로 사용할 수 있습니다. 보통은 3개의 스테이지를 사용한다고 합니다. stage 1 구조는 고정이고 stage 2 부터는 stage 2 구조를 반복해서 추론합니다. stage 2 부터는 입력이 heatmap(image feature)이 되기 때문에 stage 단계를 거칠수록 keypoint가 refinement 되는 효과를 볼 수 있습니다.\n",
    "![3](https://aiffelstaticprd.blob.core.windows.net/media/images/15.max-800x600.png)  \n",
    "사실 CPM 이 아주 좋은 방법이라고는 말하기 어렵습니다. Multi-stage 방법을 사용하기 때문에 end-to-end 로 학습이 가능하더라도 그대로 학습하는 경우는 높은 성능을 달성하기 어렵습니다. 따라서 stage 단위로 pretraining 을 한 후 다시 하나의 모델로 합쳐서 학습을 합니다. 논문을 작성하기 위해서라면 충분히 감내할 수 있지만 서비스 측면에서 바라본다면 불편한 요소라고 할 수 있습니다. 이런 문제점들은 후에 제안되는 모델들이 적극적으로 개선하고 있습니다.\n",
    "\n",
    "CPM 을 다루는 이유는 성능 때문입니다. receptive field 를 넓게 만드는 multi stage refinement 방법이 성능향상에 크게 기여한 것 같습니다.\n",
    "\n",
    "![4](https://aiffelstaticprd.blob.core.windows.net/media/images/16.max-800x600.png)\n",
    "주황색 실선이 Tompson 알고리즘입니다. CPM 에서 제안한 검정색, 회색 실선이 detection rate에서 유의미한 차이를 보이고 있는 것을 볼 수 있습니다. MPII 의 PCKh@0.5 에서 87.95% 를 달성했다고 합니다. 당시 2등보다 6.11%p 높은 성능을 보였습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-tumor",
   "metadata": {},
   "source": [
    "### Stacked Hourglass Network\n",
    "- [Stacked Hourglass Networks for Human Pose Estimation](https://arxiv.org/pdf/1603.06937.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-fourth",
   "metadata": {},
   "source": [
    "Stacked Hourglass Network 의 기본 구조는 모래시계 같은 모양으로 만들어져 있습니다. Conv layer 와 pooling 으로 이미지(또는 feature) 를 인코딩 하고 upsampling layer 를 통해 feature map 의 크기를 키우는 방향으로 decoding 합니다. feature map 크기가 작아졌다 커지는 구조여서 hourglass 라고 표현합니다.  \n",
    "\n",
    "![shn](https://aiffelstaticprd.blob.core.windows.net/media/images/17.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-contemporary",
   "metadata": {},
   "source": [
    "기존 방법들과의 가장 큰 차이점은\n",
    "\n",
    "feature map upsampling\n",
    "residual connection\n",
    "이라고 할 수 있을 것 같습니다.\n",
    "\n",
    "pooling으로 image의 global feature를 찾고 upsampling으로 local feature를 고려하는 아이디어가 hourglass의 핵심 novelty라고 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-horizon",
   "metadata": {},
   "source": [
    "### 비교\n",
    "![비교](https://aiffelstaticprd.blob.core.windows.net/media/images/23.max-800x600.png)  \n",
    "(a) : Hourglass\n",
    "(b) : CPN(cascaded pyramid networks)\n",
    "(c) : SimpleBaseline - transposed conv\n",
    "(d) : SimpleBaseline - dilated conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-aquarium",
   "metadata": {},
   "source": [
    "#### Simplebaseline 이 다른 알고리즘들에 비해 성능이 떨어지지 않지만 구조를 보면 공통점과 차이점이 있는데 무엇일까\n",
    "공통점 : high resolution → low resolution 인 encoder 와 low → high 인 decoder 구조로 이루어진 점\n",
    "\n",
    "차이점 : Hourglass 는 encoder 와 decoder 의 비율이 거의 비슷함(대칭적임). 반면 Simplebaseline 은 encoder 가 무겁고 (resnet50 등 backbone 사용) decoder 는 가벼운 모델을 사용함. (a), (b) 는 skip connection 이 있지만 (c) 는 skip connection 이 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-meter",
   "metadata": {},
   "source": [
    "#### 기존 모델들은 skip connection 을 적극적으로 사용했는데 왜 사용했을까\n",
    "pooling(strided conv) 할 때 소실되는 정보를 high level layer에서 사용해서 detail한 정보를 학습하기 위해 사용합니다. 당연히 사용할 때 성능이 더 좋을 것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-ribbon",
   "metadata": {},
   "source": [
    "### HR-NET\n",
    "![hrnet](https://aiffelstaticprd.blob.core.windows.net/media/original_images/24.png)  \n",
    "- HRNet 또한 이전 알고리즘 들과 마찬가지로 heatmap을 regression하는 방식으로 학습하고 MSE loss를 이용합니다. (특히 Simplebaseline 과 거의 유사합니다.)\n",
    "- [HR-NET github](https://github.com/leoxiaobin/deep-high-resolution-net.pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-roots",
   "metadata": {},
   "source": [
    "### SimpleBaseline 코드구조\n",
    "---\n",
    "![sb](https://aiffelstaticprd.blob.core.windows.net/media/original_images/26.png)\n",
    "- encoder : conv layers\n",
    "- decoder : deconv module + upsampling\n",
    "- [Simple Baselines for Human Pose Estimation and Tracking](https://arxiv.org/pdf/1804.06208.pdf)\n",
    "- conv model로 resnet을 사용\n",
    "- deconv module은 deconv-bn-relu 이 단계가 3개의 레이어로 이루어져있고, deconv는 256 filter size, 4x4 kernel, stride 2 로 2배씩 feature map이 커집니다.\n",
    "- 마지막 출력 레이어는 k 개의 1x1 conv layer로 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-offer",
   "metadata": {},
   "source": [
    "microsoft/human-pose-estimation.pytorch\n",
    "\n",
    "nn. 표현이 많이 등장합니다. torch.nn 으로 keras.layers 와 같이 딥러닝 모델 구성에 필요한 도구들이 정의되어 있습니다.\n",
    "\n",
    "https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/lib/models/pose_resnet.py#L29\n",
    "\n",
    "29번째 줄에서는 BasicBlock 이라는 클래스가 보이네요. keras.models 로 model 을 선언하는 것과 비슷합니다.\n",
    "\n",
    "참고로 pytorch model 에서는 사용된 layer 를 forward 함수를 통해 computational graph 를 그려줍니다.\n",
    "\n",
    "https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/lib/models/pose_resnet.py#L42\n",
    "\n",
    "forward 함수를 읽어볼까요? 앗.. 어딘가 많이 본 구조 아닌가요?\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "맞습니다. residual block 을 사용했네요.\n",
    "\n",
    "https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/lib/models/pose_resnet.py#L157\n",
    "\n",
    "Pose 메인 model 을 살펴보니 4개의 residual block 을 이용합니다. 완전 resnet 과 동일하죠?\n",
    "\n",
    "https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/lib/models/pose_resnet.py#L234\n",
    "\n",
    "forward 함수를 보면 흐름을 쉽게 알 수 있습니다.\n",
    "\n",
    "def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.deconv_layers(x)\n",
    "        x = self.final_layer(x)\n",
    "\n",
    "        return x\n",
    "resnet 을 통과한 후 deconv_layers 와 final_layer를 차례로 통과합니다.\n",
    "\n",
    "deconv layer 를 찾아보니,\n",
    "\n",
    "https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/lib/models/pose_resnet.py#L219\n",
    "\n",
    "        layers.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=self.inplanes,\n",
    "                    out_channels=planes,\n",
    "                    kernel_size=kernel,\n",
    "                    stride=2,\n",
    "                    padding=padding,\n",
    "                    output_padding=output_padding,\n",
    "                    bias=self.deconv_with_bias))\n",
    "            layers.append(nn.BatchNorm2d(planes, momentum=BN_MOMENTUM))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "transpose conv 와 bn, relu 로 이루어져있는 것을 확인했습니다.\n",
    "\n",
    "세세한 파라미터는 어디에 있을까요?\n",
    "\n",
    "EXTRA 가 자주 등장하는 것을 볼 때, 어떤 configuration 파일이 있을 것으로 짐작해 볼 수 있겠네요. repo 내에서 검색해보면 파라미터 관련 정보를 담고 있는 아래 파일을 찾을 수 있습니다.\n",
    "\n",
    "https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/experiments/coco/resnet50/256x192d256x3adam_lr1e-3.yaml#L23\n",
    "\n",
    "NUM_DECONV_LAYERS: 3\n",
    "    NUM_DECONV_FILTERS:\n",
    "    - 256\n",
    "    - 256\n",
    "    - 256\n",
    "    NUM_DECONV_KERNELS:\n",
    "    - 4\n",
    "    - 4\n",
    "    - 4\n",
    "deconv layer 의 파라미터가 아주 상세히 적혀있네요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-package",
   "metadata": {},
   "source": [
    "### SimpleBaseline - tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "shared-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "communist-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deconv module\n",
    "upconv1 = tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same')\n",
    "bn1 = tf.keras.layers.BatchNormalization()\n",
    "relu1 = tf.keras.layers.ReLU()\n",
    "upconv2 = tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same')\n",
    "bn2 = tf.keras.layers.BatchNormalization()\n",
    "relu2 = tf.keras.layers.ReLU()\n",
    "upconv3 = tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same')\n",
    "bn3 = tf.keras.layers.BatchNormalization()\n",
    "relu3 = tf.keras.layers.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "future-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = tf.keras.models.Sequential()\n",
    "    for i in range(num_deconv_layers):\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same'))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization())\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "    return seq_model\n",
    "\n",
    "upconv = _make_deconv_layer(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "excellent-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = tf.keras.layers.Conv2D(17, kernel_size=(1,1), padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "amended-installation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 256, 192, 3)]     0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Functional)        (None, None, None, 2048)  23587712  \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 64, 48, 256)       10489600  \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 48, 1)         257       \n",
      "=================================================================\n",
      "Total params: 34,077,569\n",
      "Trainable params: 34,022,913\n",
      "Non-trainable params: 54,656\n",
      "_________________________________________________________________\n",
      "(1, 256, 192, 3)\n",
      "(1, 64, 48, 1)\n",
      "tf.Tensor(\n",
      "[[[-4.81604179e-03]\n",
      "  [-4.41059377e-03]\n",
      "  [ 1.32957436e-02]\n",
      "  [ 1.11605250e-03]\n",
      "  [ 2.11111885e-02]\n",
      "  [-2.01050751e-03]\n",
      "  [ 3.79286939e-03]\n",
      "  [-5.52308932e-03]\n",
      "  [-9.19826562e-04]\n",
      "  [-4.63944161e-05]]\n",
      "\n",
      " [[-2.75840377e-03]\n",
      "  [ 7.94357434e-03]\n",
      "  [-7.31316768e-03]\n",
      "  [ 3.30407023e-02]\n",
      "  [ 7.25171645e-04]\n",
      "  [ 1.53624555e-02]\n",
      "  [-6.05529919e-03]\n",
      "  [-1.33374799e-02]\n",
      "  [ 1.73668340e-02]\n",
      "  [-2.77071493e-03]]\n",
      "\n",
      " [[-5.32525172e-03]\n",
      "  [ 1.07136006e-02]\n",
      "  [ 2.69573014e-02]\n",
      "  [ 8.27873778e-03]\n",
      "  [ 3.94763649e-02]\n",
      "  [-1.85759217e-02]\n",
      "  [ 5.17509244e-02]\n",
      "  [ 2.65634675e-02]\n",
      "  [ 3.13078538e-02]\n",
      "  [ 3.20184864e-02]]\n",
      "\n",
      " [[ 8.26944597e-03]\n",
      "  [ 4.03211080e-03]\n",
      "  [-1.49769271e-02]\n",
      "  [-5.20768948e-03]\n",
      "  [-5.98247070e-03]\n",
      "  [ 1.37761626e-02]\n",
      "  [-1.54685769e-02]\n",
      "  [ 1.02784182e-03]\n",
      "  [-2.47524977e-02]\n",
      "  [ 6.54974487e-04]]\n",
      "\n",
      " [[ 1.32904202e-03]\n",
      "  [-2.57489942e-02]\n",
      "  [ 1.57702081e-02]\n",
      "  [ 2.09341440e-02]\n",
      "  [ 3.43635827e-02]\n",
      "  [-1.70693435e-02]\n",
      "  [ 4.63939495e-02]\n",
      "  [ 2.91000050e-03]\n",
      "  [ 7.05111176e-02]\n",
      "  [-9.50527750e-03]]\n",
      "\n",
      " [[-2.04642676e-03]\n",
      "  [ 1.71854720e-02]\n",
      "  [-1.60733163e-02]\n",
      "  [ 2.90386984e-03]\n",
      "  [ 8.27407185e-03]\n",
      "  [-4.96383058e-03]\n",
      "  [ 4.04843315e-03]\n",
      "  [ 6.02499023e-03]\n",
      "  [ 3.10265459e-04]\n",
      "  [ 1.16302026e-02]]\n",
      "\n",
      " [[-4.12220508e-03]\n",
      "  [-1.19106658e-03]\n",
      "  [ 4.28660810e-02]\n",
      "  [ 2.27451567e-02]\n",
      "  [ 7.57244006e-02]\n",
      "  [ 3.08650788e-02]\n",
      "  [ 4.72808629e-02]\n",
      "  [-8.82425811e-03]\n",
      "  [ 6.76924363e-02]\n",
      "  [-1.23461001e-02]]\n",
      "\n",
      " [[ 2.97467271e-03]\n",
      "  [-1.71488747e-02]\n",
      "  [ 8.91594868e-03]\n",
      "  [ 7.83853792e-03]\n",
      "  [ 4.21351753e-04]\n",
      "  [ 1.24103073e-02]\n",
      "  [-3.61196231e-03]\n",
      "  [ 2.57244743e-02]\n",
      "  [ 1.76830795e-02]\n",
      "  [-5.91925345e-04]]\n",
      "\n",
      " [[ 1.52518768e-02]\n",
      "  [-3.63619775e-02]\n",
      "  [ 3.14954706e-02]\n",
      "  [-2.45879069e-02]\n",
      "  [ 6.52171522e-02]\n",
      "  [-1.46953454e-02]\n",
      "  [ 2.45473795e-02]\n",
      "  [ 1.39798075e-02]\n",
      "  [ 1.53868711e-02]\n",
      "  [ 5.25431428e-03]]\n",
      "\n",
      " [[-1.17529258e-02]\n",
      "  [ 7.21830642e-03]\n",
      "  [-5.38428547e-03]\n",
      "  [ 2.66159363e-02]\n",
      "  [-4.90348190e-02]\n",
      "  [-2.62436899e-03]\n",
      "  [ 3.69334072e-02]\n",
      "  [ 7.08898297e-04]\n",
      "  [-7.76733365e-03]\n",
      "  [-9.69688874e-04]]], shape=(10, 10, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = keras.models.Sequential()\n",
    "    for i in range(num_deconv_layers):\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same'))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization())\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "    return seq_model\n",
    "\n",
    "resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')\n",
    "upconv = _make_deconv_layer(3)\n",
    "final_layer = tf.keras.layers.Conv2D(1, kernel_size=(1,1), padding='same')\n",
    "\n",
    "# input :  192x256\n",
    "# output : 48x64\n",
    "inputs = keras.Input(shape=(256, 192, 3))\n",
    "x = resnet(inputs)\n",
    "x = upconv(x)\n",
    "out = final_layer(x)\n",
    "model = keras.Model(inputs, out)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# np_input = np.zeros((1,256,192,3), dtype=np.float32)\n",
    "np_input = np.random.randn(1,256,192,3)\n",
    "np_input = np.zeros((1,256,192,3), dtype=np.float32)\n",
    "tf_input = tf.convert_to_tensor(np_input, dtype=np.float32)\n",
    "print (tf_input.shape) # TensorShape([1,256,192,3])\n",
    "\n",
    "tf_output = model(tf_input)\n",
    "\n",
    "print (tf_output.shape)\n",
    "print (tf_output[0,:10,:10,:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
